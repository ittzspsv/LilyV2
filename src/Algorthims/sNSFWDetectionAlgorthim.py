import json
import unicodedata
import re
from rapidfuzz import process

SUSPICIOUS_SYMBOLS = {'@', 'â‚¬', '$', '%', '&', '!', 'Â¥', 'Â¢', 'Â¤'}

leet_dict = {
    '0': 'o', '1': 'i', '2': 'z', '3': 'e', '4': 'a', '5': 's',
    '6': 'g', '7': 't', '8': 'b', '9': 'g',
    '@': 'a', 'â‚¬': 'e', '$': 's', '!': 'i', '|': 'l', 'Â£': 'l',
    '+': 't', '(': 'c', '{': 'c', '[': 'c', ')': 'c', '}': 'c', ']': 'c',
    'Â¥': 'y', 'Â¢': 'c', 'Â¤': 'o', '^': 'v', '&': 'e', '*': 'x', 
}
homoglyph_map = {
    'ï¼¡': 'A', 'ï¼¢': 'B', 'ï¼£': 'C', 'ï¼¤': 'D', 'ï¼¥': 'E', 'ï¼¦': 'F',
    'ï¼§': 'G', 'ï¼¨': 'H', 'ï¼©': 'I', 'ï¼ª': 'J', 'ï¼«': 'K', 'ï¼¬': 'L',
    'ï¼­': 'M', 'ï¼®': 'N', 'ï¼¯': 'O', 'ï¼°': 'P', 'ï¼±': 'Q', 'ï¼²': 'R',
    'ï¼³': 'S', 'ï¼´': 'T', 'ï¼µ': 'U', 'ï¼¶': 'V', 'ï¼·': 'W', 'ï¼¸': 'X',
    'ï¼¹': 'Y', 'ï¼º': 'Z',
    'ï½': 'a', 'ï½‚': 'b', 'ï½ƒ': 'c', 'ï½„': 'd', 'ï½…': 'e', 'ï½†': 'f',
    'ï½‡': 'g', 'ï½ˆ': 'h', 'ï½‰': 'i', 'ï½Š': 'j', 'ï½‹': 'k', 'ï½Œ': 'l',
    'ï½': 'm', 'ï½': 'n', 'ï½': 'o', 'ï½': 'p', 'ï½‘': 'q', 'ï½’': 'r',
    'ï½“': 's', 'ï½”': 't', 'ï½•': 'u', 'ï½–': 'v', 'ï½—': 'w', 'ï½˜': 'x',
    'ï½™': 'y', 'ï½š': 'z',

    'ğ€': 'A', 'ğ–†': 'a', 'ğµ': 'B', 'ğ‘': 'b', 'ğ’': 'C', 'ğ’¸': 'c',
    'ğ’Ÿ': 'D', 'ğ’¹': 'd', 'ğ¸': 'E', 'ğ‘’': 'e', 'ğ‘­': 'F', 'ğ‘“': 'f',
    'ğ‘®': 'G', 'ğ‘”': 'g', 'ğ‘¯': 'H', 'ğ’½': 'h', 'ğ¼': 'I', 'ğ‘–': 'i',
    'ğ’¥': 'J', 'ğ’¿': 'j', 'ğ¾': 'K', 'ğ‘˜': 'k', 'ğ’§': 'L', 'ğ“': 'l',
    'ğ‘€': 'M', 'ğ‘š': 'm', 'ğ’©': 'N', 'ğ“ƒ': 'n', 'ğ‘¶': 'O', 'ğ‘œ': 'o',
    'ğ’«': 'P', 'ğ“…': 'p', 'ğ’¬': 'Q', 'ğ“†': 'q', 'ğ‘…': 'R', 'ğ“‡': 'r',
    'ğ’®': 'S', 'ğ“ˆ': 's', 'ğ‘‡': 'T', 'ğ“‰': 't', 'ğ‘¼': 'U', 'ğ“Š': 'u',
    'ğ’±': 'V', 'ğ“‹': 'v', 'ğ‘¾': 'W', 'ğ“Œ': 'w', 'ğ’³': 'X', 'ğ“': 'x',
    'ğ’´': 'Y', 'ğ“': 'y', 'ğ’µ': 'Z', 'ğ“': 'z',

    'Êœ': 'h',
    'Éª': 'i',
    'á´›': 't',
    'á´': 'm',
    'á´‹': 'k',
    'É´': 'n',
    'ÊŸ': 'l',
    'Ê€': 'r',
    'Êƒ': 'sh',
    'Ê’': 'zh',
    'Ê': 'y',
    'Ê': 'k',
}

leet_multi_dict = {
    '|-|': 'h',
    '|\\|': 'n',
    '||': 'u',
    '|_': 'l',
    '(_)': 'o',
}
ZERO_WIDTH_CHARS = re.compile(r'[\u200B-\u200F\u202A-\u202E\u2060-\u206F]')

def is_regional_indicator_text(text):
    text = text.replace("ï¸", "")
    pattern = r'^(?:[\U0001F1E6-\U0001F1FF\u24B6-\u24CF\U0001F170-\U0001F189]|ğŸ…°ï¸|ğŸ…±ï¸|ğŸ†|ğŸ†‘|ğŸ†’|ğŸ†“|ğŸ†”|ğŸ†•|ğŸ†–|ğŸ†—|ğŸ†˜|ğŸ†™|ğŸ†š|â“‚ï¸|ğŸ…¾ï¸|\s)*$'
    return re.fullmatch(pattern, text) is not None

def regional_indicator_to_text(text):
    styled_letters = {
        "ğŸ…°ï¸": "A", "ğŸ…±ï¸": "B", "ğŸ†": "AB", "ğŸ†‘": "CL", "ğŸ†’": "COOL",
        "ğŸ†“": "FREE", "ğŸ†”": "ID", "ğŸ†•": "NEW", "ğŸ†–": "NG", "ğŸ†—": "OK",
        "ğŸ†˜": "SOS", "ğŸ†™": "UP", "ğŸ†š": "VS", "â“‚ï¸": "M", "ğŸ…¾ï¸": "O"
    }

    def convert_match(match):
        char = match.group(0)
        char = char.replace("ï¸", "")
        if char in styled_letters:
            return styled_letters[char]
        if '\U0001F1E6' <= char <= '\U0001F1FF':
            return chr(ord(char) - 0x1F1E6 + ord('A'))
        if '\u24B6' <= char <= '\u24CF':
            return chr(ord(char) - 0x24B6 + ord('A'))
        return char
    pattern = r'[\U0001F1E6-\U0001F1FF\u24B6-\u24CF\U0001F170-\U0001F189]|ğŸ…°ï¸|ğŸ…±ï¸|ğŸ†|ğŸ†‘|ğŸ†’|ğŸ†“|ğŸ†”|ğŸ†•|ğŸ†–|ğŸ†—|ğŸ†˜|ğŸ†™|ğŸ†š|â“‚ï¸|ğŸ…¾ï¸'

    return re.sub(pattern, convert_match, text)

def decode_leet_multi(text):
    for token, char in leet_multi_dict.items():
        text = text.replace(token, char)
    return text

def remove_spaces_in_letter_groups(text):
    def repl(m):
        return m.group(0).replace(' ', '')
    return re.sub(r'(?:[a-zA-Z] )+[a-zA-Z]', repl, text)
def normalize_text(text):
    text = regional_indicator_to_text(text)
    text = ZERO_WIDTH_CHARS.sub('', text)
    text = unicodedata.normalize('NFKC', text)
    text = ''.join(homoglyph_map.get(char, char) for char in text)
    text = decode_leet_multi(text)
    text = ''.join(leet_dict.get(char, char) for char in text)
    text = re.sub(r'[^a-zA-Z0-9\s/()]+', '', text)
    text = re.sub(r'(?<=\b[a-zA-Z]) (?=[a-zA-Z]\b)', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.lower()
    return text




def load_nsfw_words(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        nsfw_words = json.load(file)
    return set(nsfw_words)

def is_nsfw(text, nsfw_set, threshold=95):
    words = re.findall(r"\b\w+\b", text.lower())
    words = [normalize_text(sword) for sword in words]
    
    return any(process.extractOne(word, nsfw_set)[1] >= threshold for word in words)